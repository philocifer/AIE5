{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- 🤝 Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- 🤝 Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# 🤝 Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank)).\n",
        "\n",
        "> You do not need to run the following cells if you are running this notebook locally. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkgFAXWVW3wm",
        "outputId": "636db35c-f05a-4038-ec7a-02360bef2dae"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU langchain langchain-openai langchain-cohere rank_bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKqYM4Eoxcov"
      },
      "source": [
        "We're also going to be leveraging [Qdrant's](https://qdrant.tech/documentation/frameworks/langchain/) (pronounced \"Quadrant\") VectorDB in \"memory\" mode (so we can leverage it locally in our colab environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s6xav5CxYnML"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU qdrant-client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LangSmith Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIM - 13 Advanced Retrieval - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using some reviews from the 4 movies in the John Wick franchise today to explore the different retrieval strategies.\n",
        "\n",
        "These were obtained from IMDB, and are available in the [AIM Data Repository](https://github.com/AI-Maker-Space/DataRepository)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXKHcZmKzDwT"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We can simply `wget` these from GitHub.\n",
        "\n",
        "You could use any review data you wanted in this step - just be careful to make sure your metadata is aligned with your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbbSIGtzX3dS",
        "outputId": "0ce6514e-2479-4001-af24-824f987ce599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-03-02 17:13:03--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19628 (19K) [text/plain]\n",
            "Saving to: ‘john_wick_1.csv’\n",
            "\n",
            "john_wick_1.csv     100%[===================>]  19.17K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-03-02 17:13:04 (1.02 MB/s) - ‘john_wick_1.csv’ saved [19628/19628]\n",
            "\n",
            "--2025-03-02 17:13:04--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14747 (14K) [text/plain]\n",
            "Saving to: ‘john_wick_2.csv’\n",
            "\n",
            "john_wick_2.csv     100%[===================>]  14.40K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2025-03-02 17:13:04 (1.75 MB/s) - ‘john_wick_2.csv’ saved [14747/14747]\n",
            "\n",
            "--2025-03-02 17:13:04--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13888 (14K) [text/plain]\n",
            "Saving to: ‘john_wick_3.csv’\n",
            "\n",
            "john_wick_3.csv     100%[===================>]  13.56K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2025-03-02 17:13:05 (1.47 MB/s) - ‘john_wick_3.csv’ saved [13888/13888]\n",
            "\n",
            "--2025-03-02 17:13:05--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15109 (15K) [text/plain]\n",
            "Saving to: ‘john_wick_4.csv’\n",
            "\n",
            "john_wick_4.csv     100%[===================>]  14.75K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-03-02 17:13:05 (63.2 MB/s) - ‘john_wick_4.csv’ saved [15109/15109]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv -O john_wick_1.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv -O john_wick_2.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv -O john_wick_3.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv -O john_wick_4.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today.\n",
        "\n",
        "- Self-Query: Wants as much metadata as we can provide\n",
        "- Time-weighted: Wants temporal data\n",
        "\n",
        "> NOTE: While we're creating a temporal relationship based on when these movies came out for illustrative purposes, it needs to be clear that the \"time-weighting\" in the Time-weighted Retriever is based on when the document was *accessed* last - not when it was created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "documents = []\n",
        "\n",
        "for i in range(1, 5):\n",
        "  loader = CSVLoader(\n",
        "      file_path=f\"john_wick_{i}.csv\",\n",
        "      metadata_columns=[\"Review_Date\", \"Review_Title\", \"Review_Url\", \"Author\", \"Rating\"]\n",
        "  )\n",
        "\n",
        "  movie_docs = loader.load()\n",
        "  for doc in movie_docs:\n",
        "\n",
        "    # Add the \"Movie Title\" (John Wick 1, 2, ...)\n",
        "    doc.metadata[\"Movie_Title\"] = f\"John Wick {i}\"\n",
        "\n",
        "    # convert \"Rating\" to an `int`, if no rating is provided - assume 0 rating\n",
        "    doc.metadata[\"Rating\"] = int(doc.metadata[\"Rating\"]) if doc.metadata[\"Rating\"] else 0\n",
        "\n",
        "    # newer movies have a more recent \"last_accessed_at\"\n",
        "    doc.metadata[\"last_accessed_at\"] = datetime.now() - timedelta(days=4-i)\n",
        "\n",
        "  documents.extend(movie_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'john_wick_1.csv', 'row': 0, 'Review_Date': '6 May 2015', 'Review_Title': ' Kinetic, concise, and stylish; John Wick kicks ass.\\n', 'Review_Url': '/review/rw3233896/?ref_=tt_urv', 'Author': 'lnvicta', 'Rating': 8, 'Movie_Title': 'John Wick 1', 'last_accessed_at': datetime.datetime(2025, 2, 27, 17, 13, 6, 696577)}, page_content=\": 0\\nReview: The best way I can describe John Wick is to picture Taken but instead of Liam Neeson it's Keanu Reeves and instead of his daughter it's his dog. That's essentially the plot of the movie. John Wick (Reeves) is out to seek revenge on the people who took something he loved from him. It's a beautifully simple premise for an action movie - when action movies get convoluted, they get bad i.e. A Good Day to Die Hard. John Wick gives the viewers what they want: Awesome action, stylish stunts, kinetic chaos, and a relatable hero to tie it all together. John Wick succeeds in its simplicity.\")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"JohnWick\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"JohnWick\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-3.5-turbo` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "# Changed to model with larger token limit to prevent errors when invoking some retriever chains\n",
        "chat_model = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [],
      "source": [
        "# naive_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [],
      "source": [
        "# naive_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [],
      "source": [
        "# naive_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [],
      "source": [
        "# bm25_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [],
      "source": [
        "# bm25_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [],
      "source": [
        "# bm25_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse - but the `I don't know` isn't great!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [],
      "source": [
        "# contextual_compression_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [],
      "source": [
        "# contextual_compression_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [],
      "source": [
        "# contextual_compression_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [],
      "source": [
        "# multi_query_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [],
      "source": [
        "# multi_query_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [],
      "source": [
        "# multi_query_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = documents\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_288434/3574430551.py:8: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-qdrant package and should be used instead. To use it run `pip install -U :class:`~langchain-qdrant` and import as `from :class:`~langchain_qdrant import Qdrant``.\n",
            "  parent_document_vectorstore = Qdrant(\n"
          ]
        }
      ],
      "source": [
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = Qdrant(\n",
        "    collection_name=\"full_documents\", embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [],
      "source": [
        "# parent_document_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [],
      "source": [
        "# parent_document_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [],
      "source": [
        "# parent_document_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [],
      "source": [
        "# ensemble_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [],
      "source": [
        "# ensemble_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [],
      "source": [
        "# ensemble_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!\n",
        "\n",
        "> NOTE: You do not need to run this cell if you're running this locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dHeB-yGXneL",
        "outputId": "efc59105-518a-4134-9228-d98b8a97e08e"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU langchain_experimental"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"JohnWickSemantic\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [],
      "source": [
        "# semantic_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [],
      "source": [
        "# semantic_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [],
      "source": [
        "# semantic_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# 🤝 Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### 🏗️ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Golden Dataset using SDG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d6fff4e0e294cd6af6cbc0d658a9567",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/44 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6455665ea6374121b18d663dd3b0f774",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Node b9c28294-d160-4313-9f74-05a91478669b does not have a summary. Skipping filtering.\n",
            "Node b4b03266-ae0e-49ae-98b3-a6cd891c7fdc does not have a summary. Skipping filtering.\n",
            "Node 9c17c0cf-ef3b-4942-b556-f76d79e90eda does not have a summary. Skipping filtering.\n",
            "Node db9a4101-77ce-4e8f-8e09-9a3039a44502 does not have a summary. Skipping filtering.\n",
            "Node b342e0c5-eae5-4c3d-b639-efa5fdc1b04c does not have a summary. Skipping filtering.\n",
            "Node e2035e9b-1294-4303-b17d-d4c30530c1c7 does not have a summary. Skipping filtering.\n",
            "Node 067b3482-3ebd-4d95-80ec-5877e449a2f2 does not have a summary. Skipping filtering.\n",
            "Node 7d8cd4bc-c22d-4b9d-a57f-f75755ef2c78 does not have a summary. Skipping filtering.\n",
            "Node 2e4605d2-750e-4d0b-a055-04a017b42094 does not have a summary. Skipping filtering.\n",
            "Node 52ef3592-3a39-4492-acfd-a8b0b3ce638b does not have a summary. Skipping filtering.\n",
            "Node 111cf59a-c84b-47d5-a32d-d99ae7820501 does not have a summary. Skipping filtering.\n",
            "Node 3a426d88-cd7a-4eda-af72-f4f95823489a does not have a summary. Skipping filtering.\n",
            "Node eb8646a6-874b-4cc1-be45-1c04196a9f72 does not have a summary. Skipping filtering.\n",
            "Node 4482278f-0312-4200-b76b-064250b10f0c does not have a summary. Skipping filtering.\n",
            "Node 0b9cb30e-e8d0-4fbc-913a-aabd5c17597b does not have a summary. Skipping filtering.\n",
            "Node dc5a5ad6-440e-4850-bd96-2607d79ddd3b does not have a summary. Skipping filtering.\n",
            "Node 09e3c4a5-6266-42e2-974d-9715e30c82be does not have a summary. Skipping filtering.\n",
            "Node 535a8906-5473-48d6-8b27-e8301f94a65c does not have a summary. Skipping filtering.\n",
            "Node 216391e0-25b3-4ef1-9184-d13c750a7e28 does not have a summary. Skipping filtering.\n",
            "Node 044c1160-7b75-416c-bb71-6cc96f5cfadf does not have a summary. Skipping filtering.\n",
            "Node 9987317e-def3-41a9-984b-7eb73f2c5370 does not have a summary. Skipping filtering.\n",
            "Node 162cb32a-82ef-42af-892f-5a605db50f5c does not have a summary. Skipping filtering.\n",
            "Node c3f93349-a294-4bfa-b706-e3347279b728 does not have a summary. Skipping filtering.\n",
            "Node 74d76b99-b50b-40c8-9f08-ef8e243a45c5 does not have a summary. Skipping filtering.\n",
            "Node 0cba8df7-8437-45a6-ae34-935c67959146 does not have a summary. Skipping filtering.\n",
            "Node 2ea2c988-3ce3-4c70-af3c-ce9a282ac162 does not have a summary. Skipping filtering.\n",
            "Node cafff5b2-d256-43d5-911f-fd039dcb5759 does not have a summary. Skipping filtering.\n",
            "Node 41de14e8-4d6b-4872-b015-e9933969db5a does not have a summary. Skipping filtering.\n",
            "Node 7800d0b5-c1cd-4e13-adaa-aabdd4d900c5 does not have a summary. Skipping filtering.\n",
            "Node 7fc89c1f-2463-4b07-b248-4433155a733d does not have a summary. Skipping filtering.\n",
            "Node 350c1b4b-6885-40aa-b5c2-e01207cf53d3 does not have a summary. Skipping filtering.\n",
            "Node 54e2c1cd-18d0-476b-8cd8-518b521d31ec does not have a summary. Skipping filtering.\n",
            "Node 570ca027-75d3-4acb-b54a-85081ae44f45 does not have a summary. Skipping filtering.\n",
            "Node a16389d1-a100-44df-945f-b9f94346a16f does not have a summary. Skipping filtering.\n",
            "Node e3feedaa-aec7-4ef7-828c-135f8d1269f5 does not have a summary. Skipping filtering.\n",
            "Node a5876487-a412-43eb-be8f-52eab0b94c3b does not have a summary. Skipping filtering.\n",
            "Node 928fc4d1-f84b-438d-b671-390e0a2f81bc does not have a summary. Skipping filtering.\n",
            "Node 23204121-aa18-44ab-84b9-0c41234f0cba does not have a summary. Skipping filtering.\n",
            "Node 08a13a72-836e-4f64-997e-67dfaaf75e03 does not have a summary. Skipping filtering.\n",
            "Node d4552a0e-cce3-4676-9533-9d0df4a5c41c does not have a summary. Skipping filtering.\n",
            "Node 6c47a255-0fe9-4745-813d-04a367671564 does not have a summary. Skipping filtering.\n",
            "Node ad61b2ad-c1ee-488c-9de9-91aef54639d4 does not have a summary. Skipping filtering.\n",
            "Node d6c72989-4682-4696-817b-c12cbd6a633c does not have a summary. Skipping filtering.\n",
            "Node 99fc3bce-8d53-435f-bae3-68414f6331fd does not have a summary. Skipping filtering.\n",
            "Node 5174857f-49ef-474f-a95a-67919bd4e2fc does not have a summary. Skipping filtering.\n",
            "Node de91b393-0f64-48e6-9fee-92c492641673 does not have a summary. Skipping filtering.\n",
            "Node ddd0639f-21a8-4c17-9c83-f318b2ace2d6 does not have a summary. Skipping filtering.\n",
            "Node 7626aa71-5925-4774-9312-666277722feb does not have a summary. Skipping filtering.\n",
            "Node bbe339ae-7f1c-420e-9fea-1af4e51cefea does not have a summary. Skipping filtering.\n",
            "Node 3b8518f1-80ae-4dd3-9fa5-321054d7fc4e does not have a summary. Skipping filtering.\n",
            "Node 4483615d-615b-43df-8008-02dfe12fe208 does not have a summary. Skipping filtering.\n",
            "Node 7bdc5f7f-d570-454f-8b30-0f370e4ef126 does not have a summary. Skipping filtering.\n",
            "Node c882b8f4-e261-46e0-8bc0-95b8a2a02e87 does not have a summary. Skipping filtering.\n",
            "Node 15aef6c3-c7d6-44ca-a928-5e46f735ffd7 does not have a summary. Skipping filtering.\n",
            "Node 8b3855b3-33a8-4776-b440-bdf69c3d7f69 does not have a summary. Skipping filtering.\n",
            "Node a6aa584a-cea8-4789-82a0-2924ad3b2ef4 does not have a summary. Skipping filtering.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "521ad9c047234480b660c7e0361ac64e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/244 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d312dcdb07b240baba919403a3bc161b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying OverlapScoreBuilder:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db03b62007b04e958da9c8c523e81c9e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b01817a692d40b99502aab2ce3a7db1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Bad request: invalid \\'dotted_order\\': dotted_order ac875ea7-f2f4-4a32-8bce-5ff65417ef53 has timestamp 2025-03-02 23:15:25.69464 +0000 UTC earlier than parent timestamp 2025-03-02 23:15:25.989045 +0000 UTC for run_id:ac875ea7-f2f4-4a32-8bce-5ff65417ef53 trace_id:9083ffa8-eeb6-4d8d-aaaa-fa22875269b5 dotted_order:20250302T231525989045Z9083ffa8-eeb6-4d8d-aaaa-fa22875269b5.20250302T231525694640Zac875ea7-f2f4-4a32-8bce-5ff65417ef53 parent_run_id:9083ffa8-eeb6-4d8d-aaaa-fa22875269b5\"}\\n')\n",
            "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Bad request: invalid \\'dotted_order\\': dotted_order ac875ea7-f2f4-4a32-8bce-5ff65417ef53 has timestamp 2025-03-02 23:15:25.69464 +0000 UTC earlier than parent timestamp 2025-03-02 23:15:25.989045 +0000 UTC for run_id:41d54ed8-3853-4aee-a28e-ecdc1d4254b2 trace_id:9083ffa8-eeb6-4d8d-aaaa-fa22875269b5 dotted_order:20250302T231525989045Z9083ffa8-eeb6-4d8d-aaaa-fa22875269b5.20250302T231525694640Zac875ea7-f2f4-4a32-8bce-5ff65417ef53.20250302T231525716990Z47928131-b375-4956-9de6-23b066eb0521.20250302T231527527180Z1efb62d2-d3ea-4caf-88d5-3234ecda63fc.20250302T231527528532Z41d54ed8-3853-4aee-a28e-ecdc1d4254b2 parent_run_id:1efb62d2-d3ea-4caf-88d5-3234ecda63fc\"}\\n')\n",
            "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Bad request: invalid \\'dotted_order\\': dotted_order ac875ea7-f2f4-4a32-8bce-5ff65417ef53 has timestamp 2025-03-02 23:15:25.69464 +0000 UTC earlier than parent timestamp 2025-03-02 23:15:25.989045 +0000 UTC for run_id:44a8bea3-de09-454b-9b98-d602ab76f0a9 trace_id:9083ffa8-eeb6-4d8d-aaaa-fa22875269b5 dotted_order:20250302T231525989045Z9083ffa8-eeb6-4d8d-aaaa-fa22875269b5.20250302T231525694640Zac875ea7-f2f4-4a32-8bce-5ff65417ef53.20250302T231525726119Z8b9f7487-3dd0-4d92-adfd-2871cd90e28d.20250302T231528160096Zd57ab443-25b0-428c-bfa0-c53313832c9d.20250302T231528161490Z44a8bea3-de09-454b-9b98-d602ab76f0a9 parent_run_id:d57ab443-25b0-428c-bfa0-c53313832c9d\"}\\n')\n",
            "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Bad request: invalid \\'dotted_order\\': dotted_order ac875ea7-f2f4-4a32-8bce-5ff65417ef53 has timestamp 2025-03-02 23:15:25.69464 +0000 UTC earlier than parent timestamp 2025-03-02 23:15:25.989045 +0000 UTC for run_id:13d27bcf-1d49-466a-b6e7-5349ff32c828 trace_id:9083ffa8-eeb6-4d8d-aaaa-fa22875269b5 dotted_order:20250302T231525989045Z9083ffa8-eeb6-4d8d-aaaa-fa22875269b5.20250302T231525694640Zac875ea7-f2f4-4a32-8bce-5ff65417ef53.20250302T231525716990Z47928131-b375-4956-9de6-23b066eb0521.20250302T231529475823Z13d27bcf-1d49-466a-b6e7-5349ff32c828 parent_run_id:47928131-b375-4956-9de6-23b066eb0521\"}\\n')\n",
            "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Bad request: invalid \\'dotted_order\\': dotted_order ac875ea7-f2f4-4a32-8bce-5ff65417ef53 has timestamp 2025-03-02 23:15:25.69464 +0000 UTC earlier than parent timestamp 2025-03-02 23:15:25.989045 +0000 UTC for run_id:9f4cd8bb-e7ad-4b0d-8da4-bb8ba226e7bf trace_id:9083ffa8-eeb6-4d8d-aaaa-fa22875269b5 dotted_order:20250302T231525989045Z9083ffa8-eeb6-4d8d-aaaa-fa22875269b5.20250302T231525694640Zac875ea7-f2f4-4a32-8bce-5ff65417ef53.20250302T231525716990Z47928131-b375-4956-9de6-23b066eb0521.20250302T231530477464Z9f4cd8bb-e7ad-4b0d-8da4-bb8ba226e7bf parent_run_id:47928131-b375-4956-9de6-23b066eb0521\"}\\n')\n",
            "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Bad request: invalid \\'dotted_order\\': dotted_order ac875ea7-f2f4-4a32-8bce-5ff65417ef53 has timestamp 2025-03-02 23:15:25.69464 +0000 UTC earlier than parent timestamp 2025-03-02 23:15:25.989045 +0000 UTC for run_id:51d01e19-79b0-47e5-b38c-ce5cb83290bc trace_id:9083ffa8-eeb6-4d8d-aaaa-fa22875269b5 dotted_order:20250302T231525989045Z9083ffa8-eeb6-4d8d-aaaa-fa22875269b5.20250302T231525694640Zac875ea7-f2f4-4a32-8bce-5ff65417ef53.20250302T231525726119Z8b9f7487-3dd0-4d92-adfd-2871cd90e28d.20250302T231531606335Z51d01e19-79b0-47e5-b38c-ce5cb83290bc parent_run_id:8b9f7487-3dd0-4d92-adfd-2871cd90e28d\"}\\n')\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75e52e3d7c904aca8d34546723c8b06c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Bad request: invalid \\'dotted_order\\': dotted_order ac875ea7-f2f4-4a32-8bce-5ff65417ef53 has timestamp 2025-03-02 23:15:25.69464 +0000 UTC earlier than parent timestamp 2025-03-02 23:15:25.989045 +0000 UTC for run_id:ac875ea7-f2f4-4a32-8bce-5ff65417ef53 trace_id:9083ffa8-eeb6-4d8d-aaaa-fa22875269b5 dotted_order:20250302T231525989045Z9083ffa8-eeb6-4d8d-aaaa-fa22875269b5.20250302T231525694640Zac875ea7-f2f4-4a32-8bce-5ff65417ef53 parent_run_id:9083ffa8-eeb6-4d8d-aaaa-fa22875269b5\"}\\n')\n"
          ]
        }
      ],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator_llm = LangchainLLMWrapper(chat_model)\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "golden_dataset = generator.generate_with_langchain_docs(documents, testset_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What are the key elements that make Keanu Reev...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>Keanu Reeves' performance in John Wick stands ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What are the general public's opinions on the ...</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>The fourth installment of John Wick is scoring...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What make Keanu special in John wick?</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>Keanu is special in John Wick because he portr...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How does the portrayal of Russian mobsters con...</td>\n",
              "      <td>[: 4\\nReview: Though he no longer has a taste ...</td>\n",
              "      <td>In the film, the portrayal of Russian mobsters...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What motivates John Wick in the film?</td>\n",
              "      <td>[: 5\\nReview: Ultra-violent first entry with l...</td>\n",
              "      <td>In the original John Wick (2014), the ex-hit-m...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How does the visual style of 'John Wick: Chapt...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 8\\nReview: About 6 months ago I ...</td>\n",
              "      <td>'John Wick: Chapter 3 - Parabellum' is noted f...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>In what ways does John Wick Chapter 2 maintain...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 23\\nReview: I love me a bit of t...</td>\n",
              "      <td>John Wick Chapter 2 maintains its entertainmen...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How does the action and plot of 'John Wick Cha...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 20\\nReview: In a world where mov...</td>\n",
              "      <td>'John Wick Chapter 2' is described as relentle...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Why was John Wick 3 a disappointment despite b...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 16\\nReview: Ok, so I got back fr...</td>\n",
              "      <td>John Wick 3 was a disappointment for the revie...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How does Keanu Reeves' portrayal of John Wick ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 20\\nReview: John Wick is somethi...</td>\n",
              "      <td>Keanu Reeves' portrayal of John Wick significa...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  What are the key elements that make Keanu Reev...   \n",
              "1  What are the general public's opinions on the ...   \n",
              "2              What make Keanu special in John wick?   \n",
              "3  How does the portrayal of Russian mobsters con...   \n",
              "4              What motivates John Wick in the film?   \n",
              "5  How does the visual style of 'John Wick: Chapt...   \n",
              "6  In what ways does John Wick Chapter 2 maintain...   \n",
              "7  How does the action and plot of 'John Wick Cha...   \n",
              "8  Why was John Wick 3 a disappointment despite b...   \n",
              "9  How does Keanu Reeves' portrayal of John Wick ...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [: 3\\nReview: John wick has a very simple reve...   \n",
              "3  [: 4\\nReview: Though he no longer has a taste ...   \n",
              "4  [: 5\\nReview: Ultra-violent first entry with l...   \n",
              "5  [<1-hop>\\n\\n: 8\\nReview: About 6 months ago I ...   \n",
              "6  [<1-hop>\\n\\n: 23\\nReview: I love me a bit of t...   \n",
              "7  [<1-hop>\\n\\n: 20\\nReview: In a world where mov...   \n",
              "8  [<1-hop>\\n\\n: 16\\nReview: Ok, so I got back fr...   \n",
              "9  [<1-hop>\\n\\n: 20\\nReview: John Wick is somethi...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  Keanu Reeves' performance in John Wick stands ...   \n",
              "1  The fourth installment of John Wick is scoring...   \n",
              "2  Keanu is special in John Wick because he portr...   \n",
              "3  In the film, the portrayal of Russian mobsters...   \n",
              "4  In the original John Wick (2014), the ex-hit-m...   \n",
              "5  'John Wick: Chapter 3 - Parabellum' is noted f...   \n",
              "6  John Wick Chapter 2 maintains its entertainmen...   \n",
              "7  'John Wick Chapter 2' is described as relentle...   \n",
              "8  John Wick 3 was a disappointment for the revie...   \n",
              "9  Keanu Reeves' portrayal of John Wick significa...   \n",
              "\n",
              "                       synthesizer_name  \n",
              "0  single_hop_specifc_query_synthesizer  \n",
              "1  single_hop_specifc_query_synthesizer  \n",
              "2  single_hop_specifc_query_synthesizer  \n",
              "3  single_hop_specifc_query_synthesizer  \n",
              "4  single_hop_specifc_query_synthesizer  \n",
              "5  multi_hop_specific_query_synthesizer  \n",
              "6  multi_hop_specific_query_synthesizer  \n",
              "7  multi_hop_specific_query_synthesizer  \n",
              "8  multi_hop_specific_query_synthesizer  \n",
              "9  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "golden_dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RAGAS Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import EvaluationDataset, RunConfig, evaluate\n",
        "from ragas.metrics import context_precision, context_recall, context_entity_recall, answer_relevancy, faithfulness\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "retriever_specific_metrics = [context_precision, context_recall, context_entity_recall]\n",
        "additional_metrics = [faithfulness, answer_relevancy]\n",
        "all_metrics = retriever_specific_metrics + additional_metrics\n",
        "\n",
        "def run_ragas_evaluation(retriever_chain, dataset):\n",
        "    for test_row in dataset:\n",
        "        response = retriever_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "        test_row.eval_sample.response = response[\"response\"].content\n",
        "        test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]] \n",
        "\n",
        "    evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "    evaluator_llm = LangchainLLMWrapper(chat_model)\n",
        "\n",
        "    custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "    result = evaluate(\n",
        "        dataset=evaluation_dataset,\n",
        "        metrics=all_metrics,\n",
        "        llm=evaluator_llm,\n",
        "        run_config=custom_run_config\n",
        "    )\n",
        "\n",
        "    return result\n",
        "\n",
        "def add_results_to_df(results_df, name, result):\n",
        "    df = pd.DataFrame([{\"retriver_name\": name}])\n",
        "    df = pd.concat([df, pd.DataFrame(json.loads(f\"{result}\".replace(\"'\", '\"')), index=[0])], axis=1)\n",
        "    if results_df is None:\n",
        "        results_df = df\n",
        "    else:\n",
        "        results_df = pd.concat([results_df, df], ignore_index=True)\n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Naive\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b729bcea86234a1988ef7b01411bd447",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_precision': 0.5863, 'context_recall': 0.7500, 'context_entity_recall': 0.4900, 'faithfulness': 0.8509, 'answer_relevancy': 0.8669}\n",
            "\n",
            "==================================================\n",
            "\n",
            "Evaluating BM25\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9145c4437b9d4bce81af64efa00d4cdd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_precision': 0.3806, 'context_recall': 0.6667, 'context_entity_recall': 0.5683, 'faithfulness': 0.6464, 'answer_relevancy': 0.6760}\n",
            "\n",
            "==================================================\n",
            "\n",
            "Evaluating Contextual Compression\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92c7463567b54c52a5d429601363d511",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_precision': 0.6917, 'context_recall': 0.5333, 'context_entity_recall': 0.4300, 'faithfulness': 0.8402, 'answer_relevancy': 0.6745}\n",
            "\n",
            "==================================================\n",
            "\n",
            "Evaluating Parent Document\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Bad request: invalid \\'dotted_order\\': dotted_order 651aa1fa-cc9c-46ea-b853-1c9f1264d6d4 has timestamp 2025-03-02 23:19:05.265515 +0000 UTC earlier than parent timestamp 2025-03-02 23:19:05.973379 +0000 UTC for run_id:651aa1fa-cc9c-46ea-b853-1c9f1264d6d4 trace_id:aeb2d257-c876-4786-903a-3d947726709d dotted_order:20250302T231905973379Zaeb2d257-c876-4786-903a-3d947726709d.20250302T231905265515Z651aa1fa-cc9c-46ea-b853-1c9f1264d6d4 parent_run_id:aeb2d257-c876-4786-903a-3d947726709d\"}\\n')\n",
            "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Bad request: invalid \\'dotted_order\\': dotted_order 5ea6c993-9be6-451f-a869-7c7c5ef1b0ff has timestamp 2025-03-02 23:19:05.272228 +0000 UTC earlier than parent timestamp 2025-03-02 23:19:05.973379 +0000 UTC for run_id:5ea6c993-9be6-451f-a869-7c7c5ef1b0ff trace_id:aeb2d257-c876-4786-903a-3d947726709d dotted_order:20250302T231905973379Zaeb2d257-c876-4786-903a-3d947726709d.20250302T231905272228Z5ea6c993-9be6-451f-a869-7c7c5ef1b0ff parent_run_id:aeb2d257-c876-4786-903a-3d947726709d\"}\\n')\n",
            "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Bad request: invalid \\'dotted_order\\': dotted_order a33d0e6c-bd95-4a3a-88fd-8d34e8667c41 has timestamp 2025-03-02 23:22:12.305898 +0000 UTC earlier than parent timestamp 2025-03-02 23:22:12.314466 +0000 UTC for run_id:a33d0e6c-bd95-4a3a-88fd-8d34e8667c41 trace_id:6d5f9ba0-5618-4379-8306-10adfc7450da dotted_order:20250302T232212309886Z6d5f9ba0-5618-4379-8306-10adfc7450da.20250302T232212310417Za2765601-e7f3-472f-9990-01b0c61c00c0.20250302T232212311523Z86d0d827-8048-4cd9-975e-419cbd2b54d2.20250302T232212314466Z7d09c466-9d5d-410b-96f4-9391ab85084b.20250302T232212305898Za33d0e6c-bd95-4a3a-88fd-8d34e8667c41 parent_run_id:7d09c466-9d5d-410b-96f4-9391ab85084b\"}\\n')\n",
            "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Bad request: invalid \\'dotted_order\\': dotted_order a33d0e6c-bd95-4a3a-88fd-8d34e8667c41 has timestamp 2025-03-02 23:22:12.305898 +0000 UTC earlier than parent timestamp 2025-03-02 23:22:12.314466 +0000 UTC for run_id:14441d70-71b4-4571-9725-675c6c5182b9 trace_id:6d5f9ba0-5618-4379-8306-10adfc7450da dotted_order:20250302T232212309886Z6d5f9ba0-5618-4379-8306-10adfc7450da.20250302T232212310417Za2765601-e7f3-472f-9990-01b0c61c00c0.20250302T232212311523Z86d0d827-8048-4cd9-975e-419cbd2b54d2.20250302T232212314466Z7d09c466-9d5d-410b-96f4-9391ab85084b.20250302T232212305898Za33d0e6c-bd95-4a3a-88fd-8d34e8667c41.20250302T232212306737Z5b2ea6d4-09fa-4f57-8501-40cf207fdc0a.20250302T232219313121Z14441d70-71b4-4571-9725-675c6c5182b9 parent_run_id:5b2ea6d4-09fa-4f57-8501-40cf207fdc0a\"}\\n')\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b91285d51f2f4590880f4f4990bd09fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_precision': 0.5583, 'context_recall': 0.4333, 'context_entity_recall': 0.4467, 'faithfulness': 0.5814, 'answer_relevancy': 0.7687}\n",
            "\n",
            "==================================================\n",
            "\n",
            "Evaluating Multi-Query\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b71d9f4c7804694b8ef2648884205ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_precision': 0.6520, 'context_recall': 0.9000, 'context_entity_recall': 0.4650, 'faithfulness': 0.9411, 'answer_relevancy': 0.8627}\n",
            "\n",
            "==================================================\n",
            "\n",
            "Evaluating Ensemble\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a113a2ff674471ab12b9f46d4c1f193",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_precision': 0.6336, 'context_recall': 0.8333, 'context_entity_recall': 0.5617, 'faithfulness': 0.8978, 'answer_relevancy': 0.8750}\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>retriver_name</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_entity_recall</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Naive</td>\n",
              "      <td>0.5863</td>\n",
              "      <td>0.7500</td>\n",
              "      <td>0.4900</td>\n",
              "      <td>0.8509</td>\n",
              "      <td>0.8669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BM25</td>\n",
              "      <td>0.3806</td>\n",
              "      <td>0.6667</td>\n",
              "      <td>0.5683</td>\n",
              "      <td>0.6464</td>\n",
              "      <td>0.6760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Contextual Compression</td>\n",
              "      <td>0.6917</td>\n",
              "      <td>0.5333</td>\n",
              "      <td>0.4300</td>\n",
              "      <td>0.8402</td>\n",
              "      <td>0.6745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parent Document</td>\n",
              "      <td>0.5583</td>\n",
              "      <td>0.4333</td>\n",
              "      <td>0.4467</td>\n",
              "      <td>0.5814</td>\n",
              "      <td>0.7687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Multi-Query</td>\n",
              "      <td>0.6520</td>\n",
              "      <td>0.9000</td>\n",
              "      <td>0.4650</td>\n",
              "      <td>0.9411</td>\n",
              "      <td>0.8627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Ensemble</td>\n",
              "      <td>0.6336</td>\n",
              "      <td>0.8333</td>\n",
              "      <td>0.5617</td>\n",
              "      <td>0.8978</td>\n",
              "      <td>0.8750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            retriver_name  context_precision  context_recall  \\\n",
              "0                   Naive             0.5863          0.7500   \n",
              "1                    BM25             0.3806          0.6667   \n",
              "2  Contextual Compression             0.6917          0.5333   \n",
              "3         Parent Document             0.5583          0.4333   \n",
              "4             Multi-Query             0.6520          0.9000   \n",
              "5                Ensemble             0.6336          0.8333   \n",
              "\n",
              "   context_entity_recall  faithfulness  answer_relevancy  \n",
              "0                 0.4900        0.8509            0.8669  \n",
              "1                 0.5683        0.6464            0.6760  \n",
              "2                 0.4300        0.8402            0.6745  \n",
              "3                 0.4467        0.5814            0.7687  \n",
              "4                 0.4650        0.9411            0.8627  \n",
              "5                 0.5617        0.8978            0.8750  "
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever_chains = {\n",
        "    \"Naive\": naive_retrieval_chain,\n",
        "    \"BM25\": bm25_retrieval_chain,\n",
        "    \"Contextual Compression\": contextual_compression_retrieval_chain,\n",
        "    \"Parent Document\": parent_document_retrieval_chain,\n",
        "    \"Multi-Query\": multi_query_retrieval_chain,\n",
        "    \"Ensemble\": ensemble_retrieval_chain,\n",
        "    # \"Semantic Chunking\": semantic_retrieval_chain\n",
        "}\n",
        "\n",
        "results_df = None\n",
        "for name, retriever_chain in retriever_chains.items():\n",
        "    print(f\"Evaluating {name}\")\n",
        "    result = run_ragas_evaluation(retriever_chain.with_config({\"run_name\": name}), golden_dataset)\n",
        "    print(result)\n",
        "    results_df = add_results_to_df(results_df, name, result)\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add Latency and Cost from LangSmith"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>retriver_name</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_entity_recall</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>latency</th>\n",
              "      <th>cost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Naive</td>\n",
              "      <td>0.5863</td>\n",
              "      <td>0.7500</td>\n",
              "      <td>0.4900</td>\n",
              "      <td>0.8509</td>\n",
              "      <td>0.8669</td>\n",
              "      <td>4.05700</td>\n",
              "      <td>0.000673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BM25</td>\n",
              "      <td>0.3806</td>\n",
              "      <td>0.6667</td>\n",
              "      <td>0.5683</td>\n",
              "      <td>0.6464</td>\n",
              "      <td>0.6760</td>\n",
              "      <td>3.72900</td>\n",
              "      <td>0.000296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Contextual Compression</td>\n",
              "      <td>0.6917</td>\n",
              "      <td>0.5333</td>\n",
              "      <td>0.4300</td>\n",
              "      <td>0.8402</td>\n",
              "      <td>0.6745</td>\n",
              "      <td>4.12000</td>\n",
              "      <td>0.000302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parent Document</td>\n",
              "      <td>0.5583</td>\n",
              "      <td>0.4333</td>\n",
              "      <td>0.4467</td>\n",
              "      <td>0.5814</td>\n",
              "      <td>0.7687</td>\n",
              "      <td>3.21750</td>\n",
              "      <td>0.000205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Multi-Query</td>\n",
              "      <td>0.6520</td>\n",
              "      <td>0.9000</td>\n",
              "      <td>0.4650</td>\n",
              "      <td>0.9411</td>\n",
              "      <td>0.8627</td>\n",
              "      <td>7.18600</td>\n",
              "      <td>0.000838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Ensemble</td>\n",
              "      <td>0.6336</td>\n",
              "      <td>0.8333</td>\n",
              "      <td>0.5617</td>\n",
              "      <td>0.8978</td>\n",
              "      <td>0.8750</td>\n",
              "      <td>7.18125</td>\n",
              "      <td>0.000915</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            retriver_name  context_precision  context_recall  \\\n",
              "0                   Naive             0.5863          0.7500   \n",
              "1                    BM25             0.3806          0.6667   \n",
              "2  Contextual Compression             0.6917          0.5333   \n",
              "3         Parent Document             0.5583          0.4333   \n",
              "4             Multi-Query             0.6520          0.9000   \n",
              "5                Ensemble             0.6336          0.8333   \n",
              "\n",
              "   context_entity_recall  faithfulness  answer_relevancy  latency      cost  \n",
              "0                 0.4900        0.8509            0.8669  4.05700  0.000673  \n",
              "1                 0.5683        0.6464            0.6760  3.72900  0.000296  \n",
              "2                 0.4300        0.8402            0.6745  4.12000  0.000302  \n",
              "3                 0.4467        0.5814            0.7687  3.21750  0.000205  \n",
              "4                 0.4650        0.9411            0.8627  7.18600  0.000838  \n",
              "5                 0.5617        0.8978            0.8750  7.18125  0.000915  "
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever_stats = {\n",
        "    \"Naive\": {\"latency\": 4.057, \"cost\": 0.00067311},\n",
        "    \"BM25\": {\"latency\": 3.729, \"cost\": 0.0002965},\n",
        "    \"Contextual Compression\": {\"latency\": 4.12, \"cost\": 0.000302025},\n",
        "    \"Parent Document\": {\"latency\": 3.2175, \"cost\": 0.00020503125},\n",
        "    \"Multi-Query\": {\"latency\": 7.186, \"cost\": 0.000837795},\n",
        "    \"Ensemble\": {\"latency\": 7.18125, \"cost\": 0.00091528125},\n",
        "}\n",
        "\n",
        "for name, stats in retriever_stats.items():\n",
        "    results_df.loc[(results_df['retriver_name'] == name), 'latency'] = stats['latency']\n",
        "    results_df.loc[(results_df['retriver_name'] == name), 'cost'] = stats['cost']\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Best Retriever Analysis\n",
        "Based on the *retriever-specific* evaluation metrics and operational considerations, the optimal retriever choice ***depends on specific use case requirements***. For *precision-focused* applications requiring minimal latency, **Contextual Compression** offers the best accuracy-quality balance while maintaining moderate costs. For *recall-critical* use cases like comprehensive question answering, **Multi-Query** achieves exceptional context recall and solid precision, though at higher latency and cost. *Budget-conscious* implementations should consider **Parent Document Retrieval** (lowest cost, fastest) despite its lower recall metrics. **BM25** provides a cost-effective middle ground suitable for *general-purpose* use. The **Ensemble** method balances multiple metrics well but incurs the highest operational costs, making it most appropriate for *mission-critical* applications where accuracy justifies the expense."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
